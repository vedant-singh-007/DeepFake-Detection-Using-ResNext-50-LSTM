{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"16OYVfhAgF3q1BDLZHrPkfIi8BNASea7d","authorship_tag":"ABX9TyOaibDF688dHvjts7iO0/w1"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":11899982,"sourceType":"datasetVersion","datasetId":7480505}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nimport pandas as pd\nfrom glob import glob","metadata":{"id":"npmPCACts74t","executionInfo":{"status":"ok","timestamp":1747850255079,"user_tz":-330,"elapsed":1627,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:09:47.863910Z","iopub.execute_input":"2025-05-22T10:09:47.864551Z","iopub.status.idle":"2025-05-22T10:09:48.246477Z","shell.execute_reply.started":"2025-05-22T10:09:47.864523Z","shell.execute_reply":"2025-05-22T10:09:48.245665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Adjust the path based on actual folder location\nfake_folder = '/kaggle/input/videos/SDFVD Small-scale Deepfake Forgery Video Dataset/videos_fake'\nreal_folder = '/kaggle/input/videos/SDFVD Small-scale Deepfake Forgery Video Dataset/videos_real'\n\n# Check if paths exist\nprint(\"Fake folder exists:\", os.path.exists(fake_folder))\nprint(\"Real folder exists:\", os.path.exists(real_folder))","metadata":{"id":"r-gkIuuCxj7z","executionInfo":{"status":"ok","timestamp":1747850255095,"user_tz":-330,"elapsed":16,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"outputId":"97bab68d-0451-4579-d0a6-b275cf83bd4c","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:09:48.247156Z","iopub.execute_input":"2025-05-22T10:09:48.247544Z","iopub.status.idle":"2025-05-22T10:09:48.276651Z","shell.execute_reply.started":"2025-05-22T10:09:48.247524Z","shell.execute_reply":"2025-05-22T10:09:48.275991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from glob import glob\n\nfake_videos = sorted(glob(os.path.join(fake_folder, '*.mp4')))\nreal_videos = sorted(glob(os.path.join(real_folder, '*.mp4')))\n\nprint(\"Number of fake videos:\", len(fake_videos))\nprint(\"Number of real videos:\", len(real_videos))","metadata":{"id":"d5L4eXkOxtCq","executionInfo":{"status":"ok","timestamp":1747850255143,"user_tz":-330,"elapsed":43,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"outputId":"0282742e-b0f0-4a4f-beb5-531226d02165","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:09:48.277427Z","iopub.execute_input":"2025-05-22T10:09:48.277726Z","iopub.status.idle":"2025-05-22T10:09:48.285291Z","shell.execute_reply.started":"2025-05-22T10:09:48.277702Z","shell.execute_reply":"2025-05-22T10:09:48.284576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_videos =  real_videos+fake_videos\nlabels = [0]*len(real_videos) + [1]*len(fake_videos)  # 1 = Fake, 0 = Real\n\n# Example:\nfor video, label in zip(all_videos, labels):\n    print(f\"{video} -> {'Fake' if label == 1 else 'Real'}\")","metadata":{"id":"iV4wh2A5xwJd","executionInfo":{"status":"ok","timestamp":1747850255353,"user_tz":-330,"elapsed":172,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"outputId":"b1bcf75e-c0a1-49d1-e272-018ce037fa8a","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:09:48.286140Z","iopub.execute_input":"2025-05-22T10:09:48.286384Z","iopub.status.idle":"2025-05-22T10:09:48.305294Z","shell.execute_reply.started":"2025-05-22T10:09:48.286343Z","shell.execute_reply":"2025-05-22T10:09:48.304434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"frame_count = []\nfor video_file in all_videos:\n  cap = cv2.VideoCapture(video_file)\n  frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\nprint(\"frames are \" , frame_count)\nprint(\"Total no of video: \" , len(frame_count))\nprint('Average frame per video:',np.mean(frame_count))","metadata":{"id":"b8g-qKLuyEVx","executionInfo":{"status":"ok","timestamp":1747850371724,"user_tz":-330,"elapsed":116369,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"outputId":"7db5e82b-a4d0-4932-aa2a-60da28b3e79c","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:09:48.306205Z","iopub.execute_input":"2025-05-22T10:09:48.306461Z","iopub.status.idle":"2025-05-22T10:09:49.340284Z","shell.execute_reply.started":"2025-05-22T10:09:48.306435Z","shell.execute_reply":"2025-05-22T10:09:49.339438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output_video_folder = \"/kaggle/working/output_videos\"\nos.makedirs(output_video_folder, exist_ok=True)\n\nlabel_file_path = os.path.join(output_video_folder, \"labels.csv\")\nlabel_entries = []\n","metadata":{"id":"lF92C4jay3sS","executionInfo":{"status":"ok","timestamp":1747850371725,"user_tz":-330,"elapsed":45,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:09:49.342653Z","iopub.execute_input":"2025-05-22T10:09:49.342863Z","iopub.status.idle":"2025-05-22T10:09:49.347027Z","shell.execute_reply.started":"2025-05-22T10:09:49.342844Z","shell.execute_reply":"2025-05-22T10:09:49.346205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install facenet-pytorch --no-deps --quiet\n\n","metadata":{"id":"4FxFGGzd0pSt","executionInfo":{"status":"ok","timestamp":1747850402161,"user_tz":-330,"elapsed":30445,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"outputId":"dab3fa5f-a796-42a8-e9ac-307d0b62f209","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:09:49.347697Z","iopub.execute_input":"2025-05-22T10:09:49.347883Z","iopub.status.idle":"2025-05-22T10:09:50.841947Z","shell.execute_reply.started":"2025-05-22T10:09:49.347868Z","shell.execute_reply":"2025-05-22T10:09:50.841058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom facenet_pytorch import MTCNN\nfrom tqdm import tqdm\n\n# Initialize MTCNN face detector (GPU if available)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmtcnn = MTCNN(keep_all=True, device=device)\n\ndef process_video_to_output(video_path, output_folder, label, max_frames=150, size=(224, 224)):\n    cap = cv2.VideoCapture(video_path)\n    frames = []\n    video_name = os.path.splitext(os.path.basename(video_path))[0]\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Convert frame to PIL image and RGB\n        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n        # Detect faces\n        boxes, _ = mtcnn.detect(img)\n\n        if boxes is not None and len(boxes) > 0:\n            # Pick largest face\n            areas = [(x2 - x1) * (y2 - y1) for x1, y1, x2, y2 in boxes]\n            largest_idx = np.argmax(areas)\n            x1, y1, x2, y2 = boxes[largest_idx].astype(int)\n\n            # Crop and resize\n            face_crop = frame[y1:y2, x1:x2]\n            if face_crop.size == 0:\n                continue\n            face_crop = cv2.resize(face_crop, size)\n            frames.append(face_crop)\n\n            if len(frames) >= max_frames:\n                break\n\n    cap.release()\n\n    if len(frames) == 0:\n        print(f\"Skipping: {video_name} (no face detected)\")\n        return None\n\n    # Pad or trim\n    if len(frames) < max_frames:\n        frames += [frames[-1]] * (max_frames - len(frames))\n    else:\n        frames = frames[:max_frames]\n\n    # Save video\n    os.makedirs(output_folder, exist_ok=True)\n    output_path = os.path.join(output_folder, f\"{video_name}.mp4\")\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 25.0, size)\n\n    for frame in frames:\n        out.write(frame)\n    out.release()\n\n    print(f\"Saved: {output_path}\")\n    return video_name, label\n","metadata":{"id":"Wzs_h4Esy9JJ","executionInfo":{"status":"ok","timestamp":1747850426668,"user_tz":-330,"elapsed":24506,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:09:50.843263Z","iopub.execute_input":"2025-05-22T10:09:50.843669Z","iopub.status.idle":"2025-05-22T10:09:54.163480Z","shell.execute_reply.started":"2025-05-22T10:09:50.843634Z","shell.execute_reply":"2025-05-22T10:09:54.162871Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport cv2\nimport os\nfrom glob import glob\n# No need to import CascadeClassifier if not used.\n# from cv2 import CascadeClassifier # This import is not used and can be removed\n\nfake_folder = '/kaggle/input/videos/SDFVD Small-scale Deepfake Forgery Video Dataset/videos_fake'\nreal_folder = '/kaggle/input/videos/SDFVD Small-scale Deepfake Forgery Video Dataset/videos_real'\n\nfake_videos = sorted(glob(os.path.join(fake_folder, '*.mp4')))\nreal_videos = sorted(glob(os.path.join(real_folder, '*.mp4')))\n\nall_video_paths = [(path, 1) for path in fake_videos] + [(path, 0) for path in real_videos]\n\n\nfor path, label in tqdm(all_video_paths):\n    try:\n        result = process_video_to_output(path, output_video_folder, label)\n        if result:\n            video_name, label = result\n            label_entries.append((video_name, label))\n    except Exception as e:\n        # Print the path of the video file that caused the error\n        print(f\"\\nError processing video: {path}\")\n        print(f\"Error details: {e}\")\n        # Continue to the next video file\n        continue","metadata":{"id":"Nj54e8cYzAZy","outputId":"bbe2b57e-4b10-4673-df45-e3a7bb1f906f","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:09:54.164256Z","iopub.execute_input":"2025-05-22T10:09:54.164592Z","iopub.status.idle":"2025-05-22T10:50:25.345121Z","shell.execute_reply.started":"2025-05-22T10:09:54.164573Z","shell.execute_reply":"2025-05-22T10:50:25.344337Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_labels = pd.DataFrame(label_entries, columns=['video_name', 'label'])\ndf_labels.to_csv(label_file_path, index=False)","metadata":{"id":"wuCEzQ9y39nq","executionInfo":{"status":"ok","timestamp":1747850426870,"user_tz":-330,"elapsed":45,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:50:25.345954Z","iopub.execute_input":"2025-05-22T10:50:25.346468Z","iopub.status.idle":"2025-05-22T10:50:25.353525Z","shell.execute_reply.started":"2025-05-22T10:50:25.346447Z","shell.execute_reply":"2025-05-22T10:50:25.352840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_labels = df_labels.drop_duplicates()\ndf_labels = df_labels.reset_index(drop=True)\n","metadata":{"id":"xC-ySCH14CDc","executionInfo":{"status":"ok","timestamp":1747850427018,"user_tz":-330,"elapsed":146,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"outputId":"12de7c0e-c37f-4843-c3ca-d634f06af48b","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:50:25.354183Z","iopub.execute_input":"2025-05-22T10:50:25.354423Z","iopub.status.idle":"2025-05-22T10:50:25.370579Z","shell.execute_reply.started":"2025-05-22T10:50:25.354396Z","shell.execute_reply":"2025-05-22T10:50:25.369866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X=df_labels['video_name']\ny=df_labels['label']","metadata":{"id":"VQZnZ0H74IEY","executionInfo":{"status":"ok","timestamp":1747850427038,"user_tz":-330,"elapsed":18,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:50:25.371631Z","iopub.execute_input":"2025-05-22T10:50:25.371800Z","iopub.status.idle":"2025-05-22T10:50:25.383692Z","shell.execute_reply.started":"2025-05-22T10:50:25.371787Z","shell.execute_reply":"2025-05-22T10:50:25.383052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","metadata":{"id":"czbQ_6oz4Kev","executionInfo":{"status":"error","timestamp":1747850430414,"user_tz":-330,"elapsed":3375,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"outputId":"d5762cd6-4c32-4d5f-b975-06d38f8decaa","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:50:25.384539Z","iopub.execute_input":"2025-05-22T10:50:25.385026Z","iopub.status.idle":"2025-05-22T10:50:25.862577Z","shell.execute_reply.started":"2025-05-22T10:50:25.385000Z","shell.execute_reply":"2025-05-22T10:50:25.862016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = X_train.reset_index(drop=True)\nX_test = X_test.reset_index(drop=True)","metadata":{"id":"dEb9ECX-4Nm3","executionInfo":{"status":"aborted","timestamp":1747850430419,"user_tz":-330,"elapsed":27,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:50:25.863228Z","iopub.execute_input":"2025-05-22T10:50:25.863572Z","iopub.status.idle":"2025-05-22T10:50:25.867888Z","shell.execute_reply.started":"2025-05-22T10:50:25.863553Z","shell.execute_reply":"2025-05-22T10:50:25.867350Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train","metadata":{"id":"cWxZI3NL4Q1p","executionInfo":{"status":"aborted","timestamp":1747850430420,"user_tz":-330,"elapsed":24,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:50:25.868522Z","iopub.execute_input":"2025-05-22T10:50:25.868712Z","iopub.status.idle":"2025-05-22T10:50:25.886969Z","shell.execute_reply.started":"2025-05-22T10:50:25.868696Z","shell.execute_reply":"2025-05-22T10:50:25.886265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import transforms\nim_size = 224\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((im_size, im_size)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n\ntest_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((im_size, im_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean, std)\n])\n","metadata":{"id":"H6j6zoh14UCI","executionInfo":{"status":"aborted","timestamp":1747850430420,"user_tz":-330,"elapsed":22,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:11:34.990924Z","iopub.execute_input":"2025-05-22T11:11:34.991568Z","iopub.status.idle":"2025-05-22T11:11:34.996863Z","shell.execute_reply.started":"2025-05-22T11:11:34.991544Z","shell.execute_reply":"2025-05-22T11:11:34.995999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_label_dict = dict(zip(df_labels['video_name'], df_labels['label']))","metadata":{"id":"qZi__zb34ZSW","executionInfo":{"status":"aborted","timestamp":1747850430426,"user_tz":-330,"elapsed":16,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:50:25.887673Z","iopub.execute_input":"2025-05-22T10:50:25.887879Z","iopub.status.idle":"2025-05-22T10:50:25.899661Z","shell.execute_reply.started":"2025-05-22T10:50:25.887864Z","shell.execute_reply":"2025-05-22T10:50:25.898940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_label_dict","metadata":{"id":"Z5-o-sS14bgD","executionInfo":{"status":"aborted","timestamp":1747850430427,"user_tz":-330,"elapsed":16,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:50:25.900447Z","iopub.execute_input":"2025-05-22T10:50:25.900664Z","iopub.status.idle":"2025-05-22T10:50:25.916952Z","shell.execute_reply.started":"2025-05-22T10:50:25.900643Z","shell.execute_reply":"2025-05-22T10:50:25.916407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport cv2\nimport os\nimport numpy as np\nfrom torchvision import transforms\n\nclass VideoDataSet(Dataset):\n    def __init__(self, video_paths, label_dict, sequence_length=150, transform=None):\n        self.video_paths = video_paths\n        self.label_dict = label_dict\n        self.seq_length = sequence_length\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.video_paths)\n\n    def __getitem__(self, index):\n        path = self.video_paths[index]\n        video_name = os.path.splitext(os.path.basename(path))[0]\n        label = self.label_dict.get(video_name, -1)\n\n        frames = self.extract_frames(path)\n\n        if len(frames) == 0:\n            dummy = torch.zeros((3, 224, 224))\n            frames = [dummy] * self.seq_length\n\n        video_tensor = torch.stack(frames)  # Shape: [seq_len, 3, 224, 224]\n        return video_tensor, label\n\n    def extract_frames(self, path):\n        cap = cv2.VideoCapture(path)\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        # Uniform sampling indices\n        indices = np.linspace(0, max(1, total_frames - 1), self.seq_length).astype(int)\n\n        frames = []\n        for i in indices:\n            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n            success, frame = cap.read()\n            if not success:\n                break\n\n            # Convert BGR to RGB\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n            # Apply transform\n            if self.transform:\n                frame = self.transform(frame)\n            else:\n                # Default transformation if none provided\n                frame = torch.from_numpy(cv2.resize(frame, (224, 224))).permute(2, 0, 1).float() / 255.0\n\n            frames.append(frame)\n\n        cap.release()\n\n        # Pad if fewer frames\n        while len(frames) < self.seq_length:\n            frames.append(torch.zeros((3, 224, 224)))\n\n        return frames\n","metadata":{"id":"VTPQN6xa4f5G","executionInfo":{"status":"aborted","timestamp":1747850430428,"user_tz":-330,"elapsed":179550,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:13:12.757163Z","iopub.execute_input":"2025-05-22T11:13:12.757737Z","iopub.status.idle":"2025-05-22T11:13:12.765855Z","shell.execute_reply.started":"2025-05-22T11:13:12.757711Z","shell.execute_reply":"2025-05-22T11:13:12.765289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_dataset=VideoDataSet(X_train,video_label_dict,sequence_length=150,transform=train_transform)\nX_test_dataset=VideoDataSet(X_test,video_label_dict,sequence_length=150,transform=test_transform)","metadata":{"id":"HgUIWp5Y4q1A","executionInfo":{"status":"aborted","timestamp":1747850430429,"user_tz":-330,"elapsed":179550,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:13:16.732922Z","iopub.execute_input":"2025-05-22T11:13:16.733508Z","iopub.status.idle":"2025-05-22T11:13:16.737106Z","shell.execute_reply.started":"2025-05-22T11:13:16.733480Z","shell.execute_reply":"2025-05-22T11:13:16.736310Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\ny_train_dataset=DataLoader(X_train_dataset,batch_size=4,shuffle=True)\ny_test_dataset=DataLoader(X_test_dataset,batch_size=4,shuffle=True)","metadata":{"id":"mxV6WIzF4ryk","executionInfo":{"status":"aborted","timestamp":1747850430430,"user_tz":-330,"elapsed":179550,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:13:20.632519Z","iopub.execute_input":"2025-05-22T11:13:20.632999Z","iopub.status.idle":"2025-05-22T11:13:20.636924Z","shell.execute_reply.started":"2025-05-22T11:13:20.632976Z","shell.execute_reply":"2025-05-22T11:13:20.636186Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(X_train_dataset, batch_size=4, shuffle=True)\n\nfor videos, labels in train_loader:\n    print(\"Batch video tensor shape:\", videos.shape)\n    print(\"Batch labels:\", labels)\n","metadata":{"id":"zOTKOq0A4u3o","executionInfo":{"status":"aborted","timestamp":1747850430433,"user_tz":-330,"elapsed":179552,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:13:26.426472Z","iopub.execute_input":"2025-05-22T11:13:26.426756Z","iopub.status.idle":"2025-05-22T11:13:49.926266Z","shell.execute_reply.started":"2025-05-22T11:13:26.426733Z","shell.execute_reply":"2025-05-22T11:13:49.925619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loader=DataLoader(X_test_dataset,batch_size=4,shuffle=True)\nfor videos, labels in test_loader:\n    print(\"Batch video tensor shape:\", videos.shape)\n    print(\"Batch labels:\", labels)","metadata":{"id":"vHNWnMLx40F6","executionInfo":{"status":"aborted","timestamp":1747850430434,"user_tz":-330,"elapsed":179552,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:16:00.604069Z","iopub.execute_input":"2025-05-22T11:16:00.604645Z","iopub.status.idle":"2025-05-22T11:16:06.906696Z","shell.execute_reply.started":"2025-05-22T11:16:00.604622Z","shell.execute_reply":"2025-05-22T11:16:06.906106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.models as models\nimport torch.nn as nn\n\n# Load pretrained ResNeXt\nresnext = models.resnext50_32x4d(pretrained=True)\n\n# Remove final classification layer (we want 2048-dim features)\nresnext.fc = nn.Identity()\n\n# Set to eval mode\nresnext.eval()\nresnext.cuda()  # Or move to your device\n","metadata":{"id":"e5UBnTyr43si","executionInfo":{"status":"aborted","timestamp":1747850430435,"user_tz":-330,"elapsed":179551,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:16:10.454046Z","iopub.execute_input":"2025-05-22T11:16:10.454638Z","iopub.status.idle":"2025-05-22T11:16:10.897020Z","shell.execute_reply.started":"2025-05-22T11:16:10.454612Z","shell.execute_reply":"2025-05-22T11:16:10.896421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass LSTMModel(nn.Module):\n    def __init__(self, num_classes=1, latent_dim=2048, lstm_layers=1, hidden_dim=128, bidirectional=False, dropout=0.4):\n        super(LSTMModel, self).__init__()\n        \n        self.lstm = nn.LSTM(\n            input_size=latent_dim,\n            hidden_size=hidden_dim,\n            num_layers=lstm_layers,\n            batch_first=True,\n            bidirectional=bidirectional\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n        \n        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n        \n        self.linear1 = nn.Linear(lstm_output_dim, num_classes)\n        self.sigmoid = nn.Sigmoid()\n        self.relu = nn.LeakyReLU()\n        \n    def forward(self, x):\n        # x shape: [batch_size, seq_length, latent_dim]\n        \n        # LSTM expects input [batch, seq, features]\n        lstm_out, _ = self.lstm(x)  # [B, T, hidden_dim * directions]\n        \n        # Mean pool across the sequence dimension\n        pooled = torch.mean(lstm_out, dim=1)  # [B, hidden_dim * directions]\n        \n        x = self.dropout(pooled)\n        x = self.linear1(x)    # [B, num_classes]\n        \n        if self.linear1.out_features == 1:\n            # Binary classification: apply sigmoid\n            x = self.sigmoid(x).squeeze(1)  # [B]\n        else:\n            # Multi-class classification: maybe softmax outside\n            pass\n        \n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:16:16.269191Z","iopub.execute_input":"2025-05-22T11:16:16.269827Z","iopub.status.idle":"2025-05-22T11:16:16.276104Z","shell.execute_reply.started":"2025-05-22T11:16:16.269802Z","shell.execute_reply":"2025-05-22T11:16:16.275147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef extract_resnext_features_batch(batch_videos, model, device='cuda'):\n    \"\"\"\n    Args:\n        batch_videos: Tensor [B, T, 3, 224, 224] (on CPU or GPU)\n        model: ResNeXt model with fc=Identity(), already on device\n        device: 'cuda' or 'cpu'\n\n    Returns:\n        features: Tensor [B, T, 2048] on the SAME device (GPU preferred)\n    \"\"\"\n    model.eval()\n    batch_videos = batch_videos.to(device)  # Move input to device (GPU)\n    B, T, C, H, W = batch_videos.shape\n\n    flat_videos = batch_videos.view(B * T, C, H, W)  # Flatten to [B*T, 3, 224, 224]\n    features = model(flat_videos)                     # Extract features [B*T, 2048]\n    features = features.view(B, T, -1)                # Reshape to [B, T, 2048]\n\n    return features\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:16:23.736010Z","iopub.execute_input":"2025-05-22T11:16:23.736296Z","iopub.status.idle":"2025-05-22T11:16:23.741029Z","shell.execute_reply.started":"2025-05-22T11:16:23.736275Z","shell.execute_reply":"2025-05-22T11:16:23.740275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, num_epochs, optimizer, device, resnext):\n    model.to(device)\n    model.train()\n\n    criterion = torch.nn.BCELoss()  # For binary classification\n\n    for epoch in range(num_epochs):\n        total_loss = 0.0\n        total_correct = 0\n        total_samples = 0\n\n        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n        \n        for batch_idx, (videos, labels) in enumerate(train_loader):\n            labels = labels.to(device).float()\n\n            optimizer.zero_grad()\n\n            # Extract ResNeXt features: shape = [B, T, 2048]\n            with torch.no_grad():  # Prevent gradient calc for ResNeXt\n                features = extract_resnext_features_batch(videos, resnext, device=device)\n\n            # Forward pass through LSTM classifier\n            outputs = model(features).squeeze()  # Shape: [B]\n\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # Binary predictions using threshold 0.5\n            preds = (outputs >= 0.5).float()\n\n            batch_correct = (preds == labels).sum().item()\n            total_correct += batch_correct\n            total_samples += labels.size(0)\n            total_loss += loss.item()\n\n            batch_accuracy = batch_correct / labels.size(0)\n            running_accuracy = total_correct / total_samples\n\n            print(f\"Batch {batch_idx+1}: Loss={loss.item():.4f}, Batch Acc={batch_accuracy*100:.2f}%, Running Acc={running_accuracy*100:.2f}%\")\n\n        avg_loss = total_loss / (batch_idx + 1)\n        final_accuracy = total_correct / total_samples\n        print(f\"Epoch {epoch+1} Summary: Avg Loss={avg_loss:.4f}, Accuracy={final_accuracy*100:.2f}%\")\n","metadata":{"id":"z1QzV9WB4_nR","executionInfo":{"status":"aborted","timestamp":1747850430436,"user_tz":-330,"elapsed":179550,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:16:27.566256Z","iopub.execute_input":"2025-05-22T11:16:27.566999Z","iopub.status.idle":"2025-05-22T11:16:27.573339Z","shell.execute_reply.started":"2025-05-22T11:16:27.566959Z","shell.execute_reply":"2025-05-22T11:16:27.572641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = LSTMModel().to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# assuming train_loader is ready with (videos, labels)\n\ntrain_model(model, train_loader, num_epochs=10, optimizer=optimizer, device=device,resnext=resnext)\n","metadata":{"id":"PgUosPHS5HXi","executionInfo":{"status":"aborted","timestamp":1747850430453,"user_tz":-330,"elapsed":179566,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:16:37.851249Z","iopub.execute_input":"2025-05-22T11:16:37.852015Z","iopub.status.idle":"2025-05-22T11:17:30.325755Z","shell.execute_reply.started":"2025-05-22T11:16:37.851986Z","shell.execute_reply":"2025-05-22T11:17:30.324797Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test(model, test_loader, resnext, device):\n    model.eval()\n    criterion = torch.nn.BCELoss()\n\n    running_loss = 0.0\n    running_correct = 0\n    total_samples = 0\n\n    print(\"Testing...\\n\")\n\n    with torch.no_grad():\n        batch_num = 0\n        for videos, labels in test_loader:\n            batch_num += 1\n\n            # Move to device\n            videos = videos.to(device)             # [B, T, C, H, W]\n            labels = labels.float().to(device)     # [B]\n\n            # Extract features using ResNeXt\n            features = extract_resnext_features_batch(videos, resnext, device=device)  # [B, T, 2048]\n\n            # Forward pass through LSTM-based model\n            outputs = model(features).squeeze()    # [B], probabilities\n            loss = criterion(outputs, labels)\n\n            # Predictions\n            preds = (outputs >= 0.5).float()       # [B]\n\n            # Print predictions for each sample\n            print(f\"Batch {batch_num}:\")\n            for i in range(labels.size(0)):\n                actual = int(labels[i].item())\n                predicted = int(preds[i].item())\n                probability = outputs[i].item()\n                print(f\"  Sample {total_samples + i + 1}: Predicted = {predicted}, Probability = {probability:.4f}, True = {actual}\")\n\n            # Accuracy metrics\n            batch_correct = (preds == labels).sum().item()\n            batch_accuracy = batch_correct / labels.size(0)\n            running_correct += batch_correct\n            total_samples += labels.size(0)\n            running_accuracy = running_correct / total_samples\n            running_loss += loss.item()\n\n            print(f\"  --> Loss={loss.item():.4f}, Batch Acc={batch_accuracy*100:.2f}%, Running Acc={running_accuracy*100:.2f}%\\n\")\n\n    avg_loss = running_loss / batch_num\n    print(f\"\\nTest Summary: Avg Loss={avg_loss:.4f}, Final Accuracy={running_accuracy*100:.2f}%\")\n","metadata":{"id":"ikYEthdwBe50","executionInfo":{"status":"aborted","timestamp":1747850430454,"user_tz":-330,"elapsed":179565,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:01:14.117845Z","iopub.status.idle":"2025-05-22T11:01:14.118056Z","shell.execute_reply.started":"2025-05-22T11:01:14.117951Z","shell.execute_reply":"2025-05-22T11:01:14.117961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Call the test function\ntest(model, test_loader,resnext, device)","metadata":{"id":"mKVerLOPCM6v","executionInfo":{"status":"aborted","timestamp":1747850430455,"user_tz":-330,"elapsed":179565,"user":{"displayName":"Vedant Singh","userId":"09119702006742817659"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T11:01:14.119254Z","iopub.status.idle":"2025-05-22T11:01:14.119592Z","shell.execute_reply.started":"2025-05-22T11:01:14.119431Z","shell.execute_reply":"2025-05-22T11:01:14.119449Z"}},"outputs":[],"execution_count":null}]}